<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features - Howard H Qian, Yangxiao Lu, Kejia Ren, Gaotian Wang, Ninad Khargonkar, Yu Xiang, Kaiyu Hang">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="RISeg improves unseen object instance segmentation (UOIS) using robot interaction to correct inaccurate segmentation masks. By leveraging the motion of rigid bodies during minimal, uncertainty-driven interaction, the pipeline achieves 80.7% accuracy, a 28.2% improvement over state-of-the-art UOIS methods in cluttered scenes.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Robotics, Computer Vision, Segmentation, Interactive Perception, Rigid Body Motion, Unseen Object Instance Segmentation, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Howard H. Qian, Yangxiao Lu, Kejia Ren, Gaotian Wang, Ninad Khargonkar, Yu Xiang, Kaiyu Hang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Robot PI Lab, Rice University">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="RISeg improves unseen object instance segmentation (UOIS) using robot interaction to correct inaccurate segmentation masks. By leveraging the motion of rigid bodies during minimal, uncertainty-driven interaction, the pipeline achieves 80.7% accuracy, a 28.2% improvement over state-of-the-art UOIS methods in cluttered scenes.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://howardqian201.github.io/RISegProjectPage/">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features">
  <meta name="citation_author" content="Qian, Howard H.">
  <meta name="citation_author" content="Lu, Yangxiao">
  <meta name="citation_author" content="Ren, Kejia">
  <meta name="citation_author" content="Wang, Gaotian">
  <meta name="citation_author" content="Khargonkar, Ninad">
  <meta name="citation_author" content="Xiang, Yu">
  <meta name="citation_author" content="Hang, Kaiyu">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="IEEE International Conference on Robotics and Automation (ICRA)">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2403.01731">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features - Howard H. Qian, Yangxiao Lu, Kejia Ren, Gaotian Wang, Ninad Khargonkar, Yu Xiang, Kaiyu Hang | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link rel="apple-touch-icon" href="static/images/favicon.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features",
    "description": "RISeg improves unseen object instance segmentation (UOIS) using robot interaction to correct inaccurate segmentation masks. By leveraging the motion of rigid bodies during minimal, uncertainty-driven interaction, the pipeline achieves 80.7% accuracy, a 28.2% improvement over state-of-the-art UOIS methods in cluttered scenes.",
    "author": [
      {
        "@type": "Person",
        "name": "Howard H. Qian",
        "affiliation": {
          "@type": "Organization",
          "name": "Rice University"
        }
      },
      {
        "@type": "Person",
        "name": "Yangxiao Lu",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Texas at Dallas"
        }
      },
      {
        "@type": "Person",
        "name": "Kejia Ren",
        "affiliation": {
          "@type": "Organization",
          "name": "Rice University"
        }
      },
      {
        "@type": "Person",
        "name": "Gaotian Wang",
        "affiliation": {
          "@type": "Organization",
          "name": "Rice University"
        }
      },
      {
        "@type": "Person",
        "name": "Ninad Khargonkar",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Texas at Dallas"
        }
      },
      {
        "@type": "Person",
        "name": "Yu Xiang",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Texas at Dallas"
        }
      },
      {
        "@type": "Person",
        "name": "Kaiyu Hang",
        "affiliation": {
          "@type": "Organization",
          "name": "Rice University"
        }
      }
    ],
    "datePublished": "2024-05-01",
    "publisher": {
      "@type": "Organization",
      "name": "IEEE International Conference on Robotics and Automation (ICRA)"
    },
    "url": "https://howardqian201.github.io/RISegProjectPage/",
    "image": "https://howardqian201.github.io/RISegProjectPage/static/images/favicon.png",
    "keywords": ["Robotics", "Computer Vision", "Segmentation", "Interactive Perception", "Rigid Body Motion", "Unseen Object Instance Segmentation", "AI"],
    "abstract": "In order to successfully perform manipulation tasks in new environments, such as grasping, robots must be proficient in segmenting unseen objects from the background and/or other objects. Previous works perform unseen object instance segmentation (UOIS) by training deep neural networks on large-scale data to learn RGB/RGB-D feature embeddings, where cluttered environments often result in inaccurate segmentations. We build upon these methods and introduce a novel approach to correct inaccurate segmentation, such as under-segmentation, of static image-based UOIS masks by using robot interaction and a designed body frame-invariant feature. We demonstrate that the relative linear and rotational velocities of frames randomly attached to rigid bodies due to robot interactions can be used to identify objects and accumulate corrected object-level segmentation masks. By introducing motion to regions of segmentation uncertainty, we are able to drastically improve segmentation accuracy in an uncertainty-driven manner with minimal, non-disruptive interactions (ca. 2-3 per scene). We demonstrate the effectiveness of our proposed interactive perception pipeline in accurately segmenting cluttered scenes by achieving an average object segmentation accuracy rate of 80.7%, an increase of 28.2% when compared with other state-of-the-art UOIS methods.",
    "citation": "@article{Qian2024RISeg,\n  title={RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features},\n  author={Qian, Howard H. and Lu, Yangxiao and Ren, Kejia and Wang, Gaotian and Khargonkar, Ninad and Xiang, Yu and Hang, Kaiyu},\n  journal={IEEE International Conference on Robotics and Automation (ICRA)},\n  year={2024},\n  url={https://howardqian201.github.io/RISegProjectPage/}\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://howardqian201.github.io/RISegProjectPage/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Robotics"
      },
      {
        "@type": "Thing", 
        "name": "Computer Vision"
      },
      {
        "@type": "Thing",
        "name": "Interactive Perception"
      },
      {
        "@type": "Thing",
        "name": "Object Segmentation"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Robot PI Lab, Rice University",
    "url": "https://howardqian201.github.io/RISegProjectPage/",
      "logo": "https://howardqian201.github.io/RISegProjectPage/static/images/favicon.png",
    "sameAs": [
      "https://github.com/HowardQian201"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/howard-qian/" target="_blank">Howard H. Qian</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/yangxiaolu95/" target="_blank">Yangxiao Lu</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/kejia-ren-56a053119/" target="_blank">Kejia Ren</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/gaotian-wang/" target="_blank">Gaotian Wang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/kninad/" target="_blank">Ninad Khargonkar</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/yu-xiang-639156295/" target="_blank">Yu Xiang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/kaiyu-hang-a249b1388/" target="_blank">Kaiyu Hang</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Rice University<br><sup>2</sup> University of Texas at Dallas<br>IEEE International Conference on Robotics and Automation (ICRA) 2024</span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2403.01731.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block">
                      <a href="https://www.youtube.com/watch?v=K_FU310Jm1k" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-video"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2403.01731" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-three-quarters">
          <div id="results-carousel" class="carousel results-carousel">
           <div class="item">
            <!-- TODO: Replace with description of this result -->
            <img src="static/carousel1/img1.png" alt="First research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              1. Undersegmentation of scene's end configuration by static segmentation model.
            </h2>
          </div>
          <div class="item">
            <img src="static/carousel1/img2.png" alt="Second research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
              2. Identified robot actions.
            </h2>
          </div>
          <div class="item">
            <img src="static/carousel1/img3.png" alt="Third research result visualization" loading="lazy"/>
            <h2 class="subtitle has-text-centered">
             3. The origins of sampled body frames with matched BFIFs due to scene interactions, where matched body frames share the same color.
           </h2>
         </div>
         <div class="item">
          <img src="static/carousel1/img4.png" alt="Fourth research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            4. Accurate segmentation of scene by RISeg after interactions have been completed.
          </h2>
        </div>
      </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            In order to successfully perform manipulation tasks in new environments, such as grasping, robots must be proficient in segmenting unseen objects from the background and/or other objects. Previous works perform unseen object instance segmentation (UOIS) by training deep neural networks on large-scale data to learn RGB/RGB-D feature embeddings, where cluttered environments often result in inaccurate segmentations. We build upon these methods and introduce a novel approach to correct inaccurate segmentation, such as under-segmentation, of static image-based UOIS masks by using robot interaction and a designed body frame-invariant feature. We demonstrate that the relative linear and rotational velocities of frames randomly attached to rigid bodies due to robot interactions can be used to identify objects and accumulate corrected object-level segmentation masks. By introducing motion to regions of segmentation uncertainty, we are able to drastically improve segmentation accuracy in an uncertainty-driven manner with minimal, non-disruptive interactions (ca. 2-3 per scene). We demonstrate the effectiveness of our proposed interactive perception pipeline in accurately segmenting cluttered scenes by achieving an average object segmentation accuracy rate of 80.7%, an increase of 28.2% when compared with other state-of-the-art UOIS methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Research Result Image 1 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-half has-text-centered">
          <img src="static/images/img1.png" alt="First research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            RISeg and MSMFormer (static model) segmentations of a cluttered tabletop scene throughout the interactive perception pipeline. The scene's initial state is shown after label “0”. Scene configurations and segmentation masks after push numbers 1, 2, and 3 follow the corresponding arrows. Pushes are minimally disruptive as to not break task formation and are always less than 2cm.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Research Result Image 2 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-half has-text-centered">
          <img src="static/images/img2.png" alt="Second research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            A visual representation of BFIFs. Motions of different body frames attached to the same rigid body are transformed into the same space frame twist. 
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Research Result Image 3 -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-half has-text-centered">
          <img src="static/images/img3.png" alt="Third research result visualization" loading="lazy"/>
          <h2 class="subtitle has-text-centered">
            Segmentation results of MSMFormer and RISeg across scene configurations resulting from robot actions.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>








<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{Qian2024RISeg,
  title={RISeg: Robot Interactive Object Segmentation via Body Frame-Invariant Features},
  author={Qian, Howard H. and Lu, Yangxiao and Ren, Kejia and Wang, Gaotian and Khargonkar, Ninad and Xiang, Yu and Hang, Kaiyu},
  journal={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2024},
  url={https://howardqian201.github.io/RISegProjectPage/}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
